{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A/B Testing Done Right.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZ8pmP1o+aFsIapx2s1hXy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trentinsf/AB_causal_inf/blob/master/A_B_Testing_Done_Right.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey5u3Bu7sHSG",
        "colab_type": "text"
      },
      "source": [
        "A/B testing fundamentally looks to evaluate the causal relationship between a treatment and an outcome, but before we dive into the specifics, we must understand why it's so hard to establish a causal relationship.\n",
        "\n",
        "In any given scenario, we can only observe what happens, not what would have happened had we done something else. While this seems obvious, it also means that we can never truly be certain about what caused what.\n",
        "\n",
        "So if we can never know what leads to what, what can we do? Instead of sitting in a dark room all day because nothing is for certain, we can collect information and begin to make judgments based on what we see. This is done constantly in our brains--when we notice that the sun tends to rise in the morning, we slowly begin to trust that the sun will rise again tomorrow morning. While the kind of evaluation done in our brains is imperfect and can lead to overgeneralizations and oversimplifications, it's the basis of inductive inference.\n",
        "\n",
        "###How can we make valid inferences?\n",
        "\n",
        "In the business world, we're always attempting to perfect our value offering, finding the perfect price for our product in various channels, finding the most effective product to advertise to our customers, or finding the perfect color scheme for our website to make our customers buy more. \n",
        "\n",
        "In all of these cases, we're trying to understand the impact of a given intervention (D), on an outcome (Y)--to do this, we must attempt to establish a relationship between the two variables.\n",
        "\n",
        "###Historical Approaches vs. A/B Testing\n",
        "\n",
        "Imagine you're a company that sells work boots and you're trying to figure out which kind of boots you should advertise to users when they visit your website. In this case, your intervention is the kind of boot you advertise, and your outcome variable is whether or not the customer makes a purchase. While you may be tempted to look at historical data, and count the number of customers that bought each kind of boot in the past, you would only be finding a correlation between the variables. Your past data may be plagued with non-controlled issues like periods where certain boots were sold out and unable to be purchased, periods where certain kinds of boots were being advertised more heavily, and many other confounding variables such as certain boots being in/out of fashion in varying periods. If you simply counted sales, you'd not only be finding simply a correlational relationship, but you'd also be solving for a different problem than the one you are presented with. Instead of learning what kind of boot would be the best to advertise to users on visiting your website, you're learning what boots sold the best in the past, which is related to your outcome variable but isn't the same.\n",
        "\n",
        "In order to learn what will be the most effective strategy, we must actively test each treatment. Instead of using this past data, which is marred with hundreds of confounding variables that cloud our insight, we'll actually test which one sells better. Statisticians like to call these kinds of tests RCT's (Randomized Controlled Trials), but in the business world, they're known as A/B tests.\n",
        "\n",
        "When a user visits our website, we randomly select one of three kinds of boots as their front page and track the percent of users that make a purchase.\n",
        "\n",
        "While many companies may simply look at which boot ad drove the most sales and make a decision, they're forgetting about the fundamental problem of causal inference, we don't know what each individual person would have done given another treatment. We could have simply happened to advertise one boot to people who really wanted to buy boots. \n",
        "\n",
        "###The Solution\n",
        "\n",
        "In order to truly make a causal inference, we can compare our observed results to a potential world where the treatments are known to have no impact, and see how likely our results would be if we *knew* that the treatments don't make a difference. If we can conclude that it's very unlikely that the differences observed in our treatment groups were due to variables outside of our control, we can conclude that our the variables in our control (the treatment variables) truly made a difference on our outcome.\n",
        "\n",
        "Before we continue, let's nail down some terms and assumptions.\n",
        "\n",
        "##TERMS:\n",
        "\n",
        "####Treatment:\n",
        "\n",
        "Treatment is our independent variable or the thing we are changing in a given trial to test its impact on what we care about.\n",
        "\n",
        "Our treatments, T_i, can be understood as a variable denoting which treatment is being used on which person. If the value of T_i is 1, we know that we used treatment 1 on customer i.\n",
        "\n",
        "Individuals who received the same treatment are part of a treatment group.\n",
        "\n",
        "####Observed Outcome:\n",
        "\n",
        "Our observed outcome is the outcome we saw for a given person. \n",
        "\n",
        "Our observed outcome, Y_i, can be understood as the outcome we saw  for a given individual. If the value for T_i is 0, we know that customer i ended up doing action 0. Often times, there are only two actions, 0 and 1, with the customer doing something such as purchasing or not purchasing, but the observed outcome can also be a continuous variable such as the total amount spent during the website visit.\n",
        "\n",
        "##ASSUMPTIONS:\n",
        "\n",
        "Our main assumption is called SUTVA, or Stable Unit Treatment Value Assumption. \n",
        "\n",
        "First, we need to know that the outcome of each trial is independent of the other trials. Practically, this means we need to know that one person's purchase isn't dependent on the treatment we give to someone else.\n",
        "\n",
        "Second, we must know that there's no hidden variation in our treatments. Practically, this means that we must know that each treatment would be executed exactly the same way for a given individual.\n",
        "\n",
        "Finally, we must ensure that individuals are equally likely to receive any treatment to ensure that our treatment is the only variable that distinguishes our treatment groups.\n",
        "\n",
        "###EXECUTION:\n",
        "\n",
        "While many companies simply rely on collecting a massive amount of data each time an A/B test is run so that they naturally converge to an accurate picture of reality due to the law of large numbers, inference can be done much more methodically and pragmatically.\n",
        "\n",
        "Consider an A/B tests run at scale, instead of these broad considerations, a company such as Macy's may be running constant A/B tests with hundreds of products and hundreds of potential demographics. This kind of segmentation prevents Macy's from advertising one product to every customer, regardless of their known demographics, but creates the issue of having extremely small sample sizes for their tests. Instead of being able to compare 500000 outcomes of one treatment with 500000 outcomes of another treatment (one million site visits), we now must evaluate the impact of 100 treatments on 100 demographics, meaning we the average number of samples we can collect from our dataset is only 100. If one in twenty people make a purchase, we have an average of 5 positive data points, and can't really know if these 100 people happened to like shopping more, or if the treatment was truly effective.\n",
        "\n",
        "Let's evaluate this scenario.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN_02vwPRRCX",
        "colab_type": "text"
      },
      "source": [
        "Consider a scenario where we found that when 100 people were given each treatment, 4 more people bought our product with treatment 1 than with treatment 2.\n",
        "\n",
        "In this case, this means that there were 7 purchases in treatment group 1 and 3 purchases in treatment group 2.\n",
        "\n",
        "This equates to a purchase percent of 7% with treatment group 1 and a purchase percent of 3% with treatment group 2. At this point, you may be tempted to simply go with treatment 1, as in our sample, the purchase rate seen in treatment group 1 was double that of treatment group 2.\n",
        "\n",
        "But how can we know whether or not there's a real difference between our treatments or if the difference is due to randomness outside of our control?\n",
        "\n",
        "We know that on average, our customers made purchases 5% of the time (10/200 people made a purchase), so let's simulate a scenario where each customer, regardless of their treatment, has a 5% chance of making a purchase, and evaluate how often we see results like ours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDHmG8YfPPBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_observations = 100 #we observe 100 people on our website\n",
        "num_simulations = 100000 #we'll simulate our exact situation 100,000 times\n",
        "\n",
        "p1 = .05 #probability of purchasing if given treatment 1 (5%)\n",
        "p2 = .05 #probability of purchasing if given treatment 2 (5%)\n",
        "#the probabilities are equal\n",
        "\n",
        "\"\"\"\n",
        "Below, we simulate whether or not a user to our website makes a purchase given each treatment.\n",
        "We simulate this 1000 times, assuming that in both cases there is a 5% chance that a user\n",
        "makes a purchase. We use a binomial distribution, which simulates independent events\n",
        "with a fixed probability of success and a fixed number of events. In this case, 100 events\n",
        "each with a 5% likelihood of success. We run this simulation 100,000 times.\n",
        "\"\"\"\n",
        "\n",
        "treatment_1 = scipy.stats.binom.rvs(n=num_observations, p=p1, size=num_simulations)\n",
        "treatment_2 = scipy.stats.binom.rvs(n=num_observations, p=p2, size=num_simulations)\n",
        "\n",
        "\"\"\"\n",
        "treatment_1 is a list the simulated number of purchases given a \n",
        "purchase probability of .05 and a sample size 100 customers\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1phDPFAS6gB",
        "colab_type": "text"
      },
      "source": [
        "In reality, we found that treatment 1 led to four more purchases than treatment 2, each havin. Let's see how often this happens randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_EYq7bYTJpF",
        "colab_type": "code",
        "outputId": "b0206227-ba49-447e-efb5-4eba0c6bfd08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "treatment_difference = treatment_1 - treatment_2\n",
        "\n",
        "\"\"\"\n",
        "Treatment_difference is the difference between the number of purchases in each treatment\n",
        "group. Our real treatment difference was 4. To see how likely it is that we would see\n",
        "these our observed results due to random chance, let's compare our result with the \n",
        "simulated results.\n",
        "\"\"\"\n",
        "\n",
        "count = 0\n",
        "\n",
        "for difference in treatment_difference:\n",
        "  if difference >= 4:\n",
        "    count += 1 \n",
        "\n",
        "\"\"\"\n",
        "We run through each simulated scenario and count the number of times that the difference\n",
        "between treatment group 1 and 2 is greater than or equal to our actual result.\n",
        "\"\"\"\n",
        "\n",
        "#we must divide by the number of simulations to get a percent:\n",
        "percent_randomly_observed = count/num_simulations\n",
        "\n",
        "print(\"Percent of time results as extreme or more extreme than ours were randomly generated:\", percent_randomly_observed*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percent of time results as extreme or more extreme than ours were randomly generated: 12.393 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSGQBERIYlv4",
        "colab_type": "text"
      },
      "source": [
        "There's our result: 12.39%. Even when we *know* that there's no difference between the treatment groups, about 12% of the time, due to randomness outside of our control, 4 or more people purchase when given treatment 1 and treatment 2.\n",
        "\n",
        "Thinking back to earlier, the only way we can claim that our treatment groups actually make an impact, we must be able to say that our results were very unlikely to be generated randomly (and not due to our treatments).\n",
        "\n",
        "In our case, we can't infer that treatment 1 is superior to treatment 2 because we can't infer from our data that the difference between the groups was not due to random chance. Especially when we have 100 treatments and 100 demographics, meaning we're running 10,000 smaller experiments of this type, this threshold is unacceptable.\n",
        "\n",
        "Different contexts require call for different thresholds to claim there is a causal connection. In most modern research contexts, a researcher needs to demonstrate that there's a less than 1% chance that their results would have been seen completely randomly (unrelated to their treatments). And even with those strict guidelines, many studies still end up falsely claiming that a causal relationship is present.\n",
        "\n",
        "This percent chance that our results were generated randomly is called a P value, and in business contexts, a P value 5% (.05) is required. Meaning a team must demonstrate that their results had a less than 5% of being due to random chance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6DrjGlkenWb",
        "colab_type": "text"
      },
      "source": [
        "While there are many mathematical approaches out there, such as the ANOVA test, or the T-Test, the essence of both is the same as this simulated test. They all evaluate whether or not we can make the assertion that our observed outcome was NOT randomly distributed by computing the probability of getting our observed data in a world where we know our treatments have no causal impact. These tests make something that's relatively simple in concept into a confusing mathematical calculation, so for the sake of understanding, I thought I'd demonstrate the simplest way to understand P values and A/B testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7CmMBvJhCXP",
        "colab_type": "text"
      },
      "source": [
        "FINAL DISCUSSION: P Values\n",
        "\n",
        "While we have already pointed out the problems will simply trusting the outcome of your A/B test without understanding the possibility of it being due entirely to randomness, it's equally important to discuss the modern discussion taking place about using a P value that's low enough.\n",
        "\n",
        "A P value represents the likelihood that our results, (or results more extreme) would be seen *in a world where we know that our treatments make no difference.* A p value of .05 does **not** tell us that there's a 95% chance that our treatment has a causal impact on our outcome. This distinction is crucial, as it's been empirically found that when the P value is .05, there's actually only 70-80% chance that there's a causal impact. Meaning that with a P value of .05, your test will lead you to take a misled action 20-30% of the time (Johnson et al., 2013).\n",
        "\n",
        "This same study, along with more recent research from more than 100 leading researchers suggests that a P value of .005 should be required for any meaningful argument to be made, and a P value of .001 is required for a significant connection to be made. (Benjamin et al., 2018) \n",
        "\n",
        "Take that into consideration next time you interpret your A/B test values: even the companies being dilligent and using a P value instead of simply choosing the option with the highest observed value are acting on misleading data up to 20-30% of the time.\n",
        "\n",
        "This is the reason data literacy, and an understanding of how to interpret the information provided by these test is more important than knowledge about how to execute these tests.\n",
        "\n",
        "It's the reason that in this practical data-science final I used 11 lines of code, but over 20 paragraphs of writing describing that code. I could have designed a model that took 30,000 variables into consideration and used the top of the line neural network, but without a clear understanding of what the results really mean, the model would be worthless.\n",
        "\n",
        "\n",
        "References:\n",
        "\n",
        "Benjamin, D. J., Berger, J. O., Johannesson, M., Nosek, B. A., Wagenmakers, E. J., Berk, R., ... & Cesarini, D. (2018). Redefine statistical significance. Nature Human Behaviour, 2(1), 6.)\n",
        "\n",
        "Johnson, V. E. (2013). Revised standards for statistical evidence. Proceedings of the National Academy of Sciences, 110(48), 19313-19317. https://www.pnas.org/content/110/48/19313.abstract"
      ]
    }
  ]
}